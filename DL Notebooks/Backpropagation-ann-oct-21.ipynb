{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e2dea1-7c64-4b01-a756-7b77fd7add82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514e8fe8-5d85-44cb-b3c7-d2d530917821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [[0.05,0.1],0.01]\n",
    "X,Y = data[0], data[1]\n",
    "l_rate = 0.5\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbae3ed9-a467-46ab-891a-ceca9e5dde08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, epochs = 5, l_rate = 0.01 ,input_layer = 0, input_bias = 0, hidden_layer_w = 0,output_bias = 0):\n",
    "        self.input_layer = input_layer\n",
    "        self.input_bias = input_bias\n",
    "        self.hidden_layer_w = hidden_layer_w\n",
    "        self.output_bias = output_bias\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        \n",
    "    def fit(self,x,Target):\n",
    "        # self.input_layer, self.input_bias, self.hidden_layer_w, self.output_bias = np.random.rand(2*len(x)),np.random.rand(len(x)),np.random.rand(len(x)),np.random.rand(len(x)-1)\n",
    "        self.input_layer, self.input_bias, self.hidden_layer_w, self.output_bias = np.array([[0.15,0.2],[0.25,0.3]]),[0.35,0.35],np.array([0.4,0.45]),[0.6] \n",
    "        print(f\"Model : Perceptron()  || Learning Rate = {self.l_rate} || Trained Started....\")\n",
    "        self.Backward_propagation(x,Target)\n",
    "        return f'Model : Perceptron()  || Learning Rate = {self.l_rate} || Trained Successfully......'\n",
    "    \n",
    "    def get_params(self):\n",
    "        return f\"=>>Hiddel layers: {self.hidden_layer_w},=>>input_layer :{self.input_layer},=>>epochs : {self.epochs}\"\n",
    "    \n",
    "    def Sigmoid(self,var):\n",
    "        return 1/(1.0+np.exp(-var))\n",
    "    \n",
    "    def Error(self,target,output):\n",
    "        return 0.5*(target - output)**2\n",
    "    \n",
    "    def derrived_error(self,weights,target,output):\n",
    "        # print(output*(1-output))\n",
    "        return self.l_rate*(output - target)*(output*(1-output))\n",
    "    \n",
    "    def forward_propagation(self,x_input):\n",
    "        # print(self.Sigmoid(np.dot(self.hidden_layer_w,self.Sigmoid(np.dot(x_input,self.input_layer) + self.input_bias)) + self.output_bias))\n",
    "        return self.Sigmoid(np.dot(self.hidden_layer_w,self.Sigmoid(np.dot(x_input,self.input_layer) + self.input_bias)) + self.output_bias)\n",
    "    \n",
    "    def Backward_propagation(self,x,target):\n",
    "        del_w = 0\n",
    "        for i in range(self.epochs):\n",
    "            O_in_K = self.forward_propagation(x)\n",
    "            print(f\">>Epoch = {i+1}/{self.epochs} Loss :{self.Error(target,O_in_K)}\")\n",
    "            # print('Sigmoid',self.Sigmoid(O_in_K))\n",
    "            del_w = self.derrived_error(self.hidden_layer_w,target,O_in_K)\n",
    "            self.hidden_layer_w = self.hidden_layer_w - del_w\n",
    "            # print(del_w) \n",
    "            self.input_layer = self.input_layer - del_w*x \n",
    "            # print(self.input_layer)\n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11034d12-b927-4985-b262-4314eed56036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : Perceptron()  || Learning Rate = 0.5 || Trained Started....\n",
      ">>Epoch = 1/100 Loss :[0.27484039]\n",
      ">>Epoch = 2/100 Loss :[0.26329041]\n",
      ">>Epoch = 3/100 Loss :[0.25132856]\n",
      ">>Epoch = 4/100 Loss :[0.23903601]\n",
      ">>Epoch = 5/100 Loss :[0.22651157]\n",
      ">>Epoch = 6/100 Loss :[0.21386884]\n",
      ">>Epoch = 7/100 Loss :[0.20123166]\n",
      ">>Epoch = 8/100 Loss :[0.18872807]\n",
      ">>Epoch = 9/100 Loss :[0.17648355]\n",
      ">>Epoch = 10/100 Loss :[0.16461424]\n",
      ">>Epoch = 11/100 Loss :[0.15322102]\n",
      ">>Epoch = 12/100 Loss :[0.14238526]\n",
      ">>Epoch = 13/100 Loss :[0.13216642]\n",
      ">>Epoch = 14/100 Loss :[0.1226018]\n",
      ">>Epoch = 15/100 Loss :[0.11370799]\n",
      ">>Epoch = 16/100 Loss :[0.10548352]\n",
      ">>Epoch = 17/100 Loss :[0.09791228]\n",
      ">>Epoch = 18/100 Loss :[0.09096709]\n",
      ">>Epoch = 19/100 Loss :[0.08461313]\n",
      ">>Epoch = 20/100 Loss :[0.0788109]\n",
      ">>Epoch = 21/100 Loss :[0.07351866]\n",
      ">>Epoch = 22/100 Loss :[0.06869426]\n",
      ">>Epoch = 23/100 Loss :[0.06429658]\n",
      ">>Epoch = 24/100 Loss :[0.06028638]\n",
      ">>Epoch = 25/100 Loss :[0.05662688]\n",
      ">>Epoch = 26/100 Loss :[0.05328409]\n",
      ">>Epoch = 27/100 Loss :[0.0502269]\n",
      ">>Epoch = 28/100 Loss :[0.04742704]\n",
      ">>Epoch = 29/100 Loss :[0.04485899]\n",
      ">>Epoch = 30/100 Loss :[0.04249981]\n",
      ">>Epoch = 31/100 Loss :[0.0403289]\n",
      ">>Epoch = 32/100 Loss :[0.03832787]\n",
      ">>Epoch = 33/100 Loss :[0.03648025]\n",
      ">>Epoch = 34/100 Loss :[0.03477136]\n",
      ">>Epoch = 35/100 Loss :[0.03318809]\n",
      ">>Epoch = 36/100 Loss :[0.03171871]\n",
      ">>Epoch = 37/100 Loss :[0.03035277]\n",
      ">>Epoch = 38/100 Loss :[0.02908092]\n",
      ">>Epoch = 39/100 Loss :[0.02789478]\n",
      ">>Epoch = 40/100 Loss :[0.02678686]\n",
      ">>Epoch = 41/100 Loss :[0.02575042]\n",
      ">>Epoch = 42/100 Loss :[0.02477942]\n",
      ">>Epoch = 43/100 Loss :[0.02386842]\n",
      ">>Epoch = 44/100 Loss :[0.02301254]\n",
      ">>Epoch = 45/100 Loss :[0.02220735]\n",
      ">>Epoch = 46/100 Loss :[0.02144886]\n",
      ">>Epoch = 47/100 Loss :[0.02073345]\n",
      ">>Epoch = 48/100 Loss :[0.02005786]\n",
      ">>Epoch = 49/100 Loss :[0.0194191]\n",
      ">>Epoch = 50/100 Loss :[0.01881449]\n",
      ">>Epoch = 51/100 Loss :[0.01824155]\n",
      ">>Epoch = 52/100 Loss :[0.01769805]\n",
      ">>Epoch = 53/100 Loss :[0.01718194]\n",
      ">>Epoch = 54/100 Loss :[0.01669135]\n",
      ">>Epoch = 55/100 Loss :[0.01622455]\n",
      ">>Epoch = 56/100 Loss :[0.01577998]\n",
      ">>Epoch = 57/100 Loss :[0.0153562]\n",
      ">>Epoch = 58/100 Loss :[0.01495186]\n",
      ">>Epoch = 59/100 Loss :[0.01456576]\n",
      ">>Epoch = 60/100 Loss :[0.01419677]\n",
      ">>Epoch = 61/100 Loss :[0.01384384]\n",
      ">>Epoch = 62/100 Loss :[0.01350601]\n",
      ">>Epoch = 63/100 Loss :[0.01318239]\n",
      ">>Epoch = 64/100 Loss :[0.01287215]\n",
      ">>Epoch = 65/100 Loss :[0.01257454]\n",
      ">>Epoch = 66/100 Loss :[0.01228885]\n",
      ">>Epoch = 67/100 Loss :[0.0120144]\n",
      ">>Epoch = 68/100 Loss :[0.01175059]\n",
      ">>Epoch = 69/100 Loss :[0.01149685]\n",
      ">>Epoch = 70/100 Loss :[0.01125264]\n",
      ">>Epoch = 71/100 Loss :[0.01101747]\n",
      ">>Epoch = 72/100 Loss :[0.01079086]\n",
      ">>Epoch = 73/100 Loss :[0.01057239]\n",
      ">>Epoch = 74/100 Loss :[0.01036165]\n",
      ">>Epoch = 75/100 Loss :[0.01015825]\n",
      ">>Epoch = 76/100 Loss :[0.00996184]\n",
      ">>Epoch = 77/100 Loss :[0.00977209]\n",
      ">>Epoch = 78/100 Loss :[0.00958867]\n",
      ">>Epoch = 79/100 Loss :[0.0094113]\n",
      ">>Epoch = 80/100 Loss :[0.00923968]\n",
      ">>Epoch = 81/100 Loss :[0.00907357]\n",
      ">>Epoch = 82/100 Loss :[0.00891271]\n",
      ">>Epoch = 83/100 Loss :[0.00875687]\n",
      ">>Epoch = 84/100 Loss :[0.00860583]\n",
      ">>Epoch = 85/100 Loss :[0.00845938]\n",
      ">>Epoch = 86/100 Loss :[0.00831733]\n",
      ">>Epoch = 87/100 Loss :[0.00817949]\n",
      ">>Epoch = 88/100 Loss :[0.00804568]\n",
      ">>Epoch = 89/100 Loss :[0.00791574]\n",
      ">>Epoch = 90/100 Loss :[0.00778951]\n",
      ">>Epoch = 91/100 Loss :[0.00766684]\n",
      ">>Epoch = 92/100 Loss :[0.00754759]\n",
      ">>Epoch = 93/100 Loss :[0.00743162]\n",
      ">>Epoch = 94/100 Loss :[0.00731881]\n",
      ">>Epoch = 95/100 Loss :[0.00720904]\n",
      ">>Epoch = 96/100 Loss :[0.00710219]\n",
      ">>Epoch = 97/100 Loss :[0.00699814]\n",
      ">>Epoch = 98/100 Loss :[0.00689681]\n",
      ">>Epoch = 99/100 Loss :[0.00679807]\n",
      ">>Epoch = 100/100 Loss :[0.00670185]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model : Perceptron()  || Learning Rate = 0.5 || Trained Successfully......'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = NeuralNetwork(epochs = epoch,l_rate = 0.5)\n",
    "r.fit(X,Y)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
